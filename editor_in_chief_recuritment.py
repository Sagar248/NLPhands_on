# -*- coding: utf-8 -*-
"""Editor-In-Chief_Recuritment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10UERBcTyTfw0I4Z0x7JpAaDB45UYwf7J

# **PRE-PROCESSING**
"""

import nltk
nltk.download("popular")
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

"""Import text file"""

from google.colab import files
uploaded = files.upload()

#NOTE: Two text files SixYearAndCounting.txt and SixYearsAndCountingNew.txt are similar documents.
#It is because, by doing that our recuritment model can be tested, that whether it detects the similarity or not.

doc_0 = open('GettingSaucyAboutFood.txt','r').read()
doc_1 = open('SixYearsAndCounting.txt','r').read()
doc_2 = open('TrainToNowhere.txt','r').read()
doc_3 = open('WhatDreamsMayCome.txt','r').read()
doc_4 = open('SixYearsAndCountingNew.txt','r').read()

all_doc = [doc_0,doc_1,doc_2,doc_3,doc_4]

"""Tokenization"""

#tokenising the text files
from nltk import word_tokenize
words0 = word_tokenize(doc_0)
words1 = word_tokenize(doc_1)
words2 = word_tokenize(doc_2)
words3 = word_tokenize(doc_3)
words4 = word_tokenize(doc_4)

#Conversion to lower case for cosine similarities and future convenience
print(words0,"\n",words1,"\n",words2,"\n",words3,"\n",words4,"\n")
words0_new = [word.lower() for word in words0]
words1_new = [word.lower() for word in words1]
words2_new = [word.lower() for word in words2]
words3_new = [word.lower() for word in words3]
words4_new = [word.lower() for word in words4]
print(words0_new,"\n",words1_new,"\n",words2_new,"\n",words3_new,"\n",words4_new)

#tokeninzing with word boundaries may cause an issue so we can remove the punctuations

#using punctuation to remove punctuations ;-) lol
import string
print(string.punctuation)

#text without punctuation with sample text_p_1 to show how it works
text_p_0 = "".join([char for char in doc_0 if char not in string.punctuation])
print(text_p_0)

text_p_0 = " ".join([char for char in words0_new if char not in string.punctuation])
text_p_1 = " ".join([char for char in words1_new if char not in string.punctuation])
text_p_2 = " ".join([char for char in words2_new if char not in string.punctuation])
text_p_3 = " ".join([char for char in words3_new if char not in string.punctuation])
text_p_4 = " ".join([char for char in words4_new if char not in string.punctuation])

#Tokenization after removing punctuation
words0_t = word_tokenize(text_p_0)
words1_t = word_tokenize(text_p_1)
words2_t = word_tokenize(text_p_2)
words3_t = word_tokenize(text_p_3)
words4_t = word_tokenize(text_p_4)
print(words0_t,"\n",words1_t,"\n",words2_t,"\n",words3_t,"\n",words4_t)

"""Removal of Stop Words"""

# removing stopwords like i me myself we etc
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
print("The stopwords are",stop_words)
words0f = [word for word in words0_t if word not in stop_words]
words1f = [word for word in words1_t if word not in stop_words]
words2f = [word for word in words2_t if word not in stop_words]
words3f = [word for word in words3_t if word not in stop_words]
words4f = [word for word in words4_t if word not in stop_words]
print(words0f,"\n",words1f,"\n",words2f,"\n",words3f,"\n",words4f)

"""Lemmatization"""

from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()

lem0 = [lemmatizer.lemmatize(word) for word in words0f]
lem1 = [lemmatizer.lemmatize(word) for word in words1f]
lem2 = [lemmatizer.lemmatize(word) for word in words2f]
lem3 = [lemmatizer.lemmatize(word) for word in words3f]
lem4 = [lemmatizer.lemmatize(word) for word in words4f]

print(lem0,"\n",lem1,"\n",lem2,"\n",lem3,"\n",lem4)

"""Stemming with POS Tagging"""

from nltk import LancasterStemmer
lc = LancasterStemmer()
stemmed0 = [lc.stem(word) for word in words0f]
stemmed1 = [lc.stem(word) for word in words1f]
stemmed2 = [lc.stem(word) for word in words2f]
stemmed3 = [lc.stem(word) for word in words3f]
stemmed4 = [lc.stem(word) for word in words4f]

print(stemmed0,"\n",stemmed1,"\n", stemmed2,"\n", stemmed3,"\n", stemmed4)

from nltk import pos_tag
pos0 = pos_tag(stemmed0)
pos1 = pos_tag(stemmed1)
pos2 = pos_tag(stemmed2)
pos3 = pos_tag(stemmed3)
pos4 = pos_tag(stemmed4)
print(pos0,"\n",pos1,"\n", pos2,"\n", pos3,"\n", pos4)

"""# **EVALUATION METRICS**

Lexical Richness
"""

!pip install lexicalrichness

from lexicalrichness import LexicalRichness

words = []
ttr = []
rttr = []
cttr = []
msttr = []
mattr = []
n = len(all_doc)
for i in range(n):
  lex = LexicalRichness(all_doc[i])
  print("Lexical Richness Assesment of text-", (i+1), " is:")
  print("Number of words: ", lex.words)
  words.append(lex.words)
  print("Type Token Ratio: ", lex.ttr)
  ttr.append(lex.ttr)
  print("Root type-token ratio: ", lex.rttr)
  rttr.append(lex.rttr)
  print("Corrected type-token ratio: ", lex.cttr, "\n")
  cttr.append(lex.cttr)

print("Final Insights: \nAverage number of words in the texts:", sum(words)/n,
      "\nAverage type-token ratio: ", sum(ttr)/n,
      "\nAverage root type-token ratio: ", sum(rttr)/n,
      "\nAverage corrected type-token ratio: ", sum(cttr)/n,
      )

"""Reading Index"""

from nltk.tokenize import sent_tokenize, word_tokenize

cont = 'yes'
vowels = ["a", "e", "i", "o", "u"]
endings = ["ed", "e", "es"]
reading_scores = {}

while(cont=="yes" or cont=="y"):
  name = input("Enter name of article: ")
  file_name = ''.join(name.title().split(" ")) + '.txt'
  try:
    article = open(file_name, 'r')
    text = article.read().lower()
    num_sentences = len(sent_tokenize(text))
    syllables = 0
    words = [word for word in word_tokenize(text) if word.isalpha()]
    num_words = len(words)
    for word in words:
      for vowel in vowels:
        syllables += word.count(vowel)
      for end in endings:
        if word.endswith(end) and (word.endswith('le')==False):
          syllables -= 1
    FRE = round(206.835 - (1.015*(num_words/num_sentences)) - (84.6 * (syllables/num_words)))
    reading_scores[name] = FRE
  except:
    print("Sorry this file does not exist in the database")
  cont = input("Any more articles to analyse? (yes-y,no-n): ")
  cont=cont.lower()

reading_scores

avg_reading_score = 0
for article in reading_scores:
  avg_reading_score += reading_scores[article]

avg_reading_score = round(avg_reading_score/len(reading_scores))
print(f"Average reading score: {avg_reading_score}")

"""Cosine Similarity"""

from sklearn.feature_extraction.text import TfidfVectorizer
import string

stemmer = nltk.stem.porter.PorterStemmer()
remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)

def stem_tokens(tokens):
    return [stemmer.stem(item) for item in tokens]

def normalize(text):
    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))

vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')

def cosine_sim(text1, text2):
    tfidf = vectorizer.fit_transform([text1, text2])
    return ((tfidf * tfidf.T).A)[0,1]

import glob
documents_list = glob.glob('./*.txt')
documents_list

documents_instances_list = []
for i in range(len(documents_list)):
  with open(documents_list[i]) as e:
    documents_instances_list.append(e.read())

from itertools import combinations

numbers = range(0, len(documents_instances_list))
k = list(combinations(numbers, 2))
m = lambda s: s.strip('./')
document_map = dict(zip(numbers, list(map(m, documents_list))))

print(document_map)
k

for i in range(len(k)):
  first_doc = k[i][0]
  second_doc = k[i][1]
  print("Document Similarity between document {}({}) and {}({}) is : \t".format(first_doc,document_map[first_doc], second_doc, document_map[second_doc]),cosine_sim(documents_instances_list[first_doc], documents_instances_list[second_doc]))

"""Jaccard Similarity"""

from __future__ import division
import string
import math

tokenize = lambda doc: doc.lower().split(" ")

doc_0 = open('GettingSaucyAboutFood.txt','r').read()
doc_1 = open('SixYearsAndCounting.txt','r').read()
doc_2 = open('TrainToNowhere.txt','r').read()
doc_3 = open('WhatDreamsMayCome.txt','r').read()
doc_4 = open('SixYearsAndCountingNew.txt','r').read()

all_doc = [doc_0,doc_1,doc_2,doc_3,doc_4]

tokenized_documents = [tokenize(d) for d in all_doc] # tokenized docs
all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])

def jaccard_similarity(query, document):
    intersection = set(query).intersection(set(document))
    union = set(query).union(set(document))
    return len(intersection)/len(union)

for i in range(len(all_doc)):
  for j in range(i+1,len(all_doc)):
    x =  jaccard_similarity(tokenized_documents[i],tokenized_documents[j])
    print("Jaccard Similarity between doc_",i," and doc_",j," is :",x)